{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b5345a-2f50-4aec-861c-b48b1941a074",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# One-Class SVM(Support Vector Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebffc6c9-3733-46d4-b491-e8027b3a4947",
   "metadata": {},
   "source": [
    "## What is SVM?\n",
    "Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification, regression, and outlier detection tasks. \n",
    "\n",
    "The key idea behind SVM is to find the optimal hyperplane that best separates the classes (categories) in the feature space. This hyperplane is chosen such that it maximizes the margin, which is the distance between the hyperplane and the nearest data points from each class, known as support vectors.\n",
    "\n",
    "Here are some important concepts and characteristics of SVM:\n",
    "\n",
    "1. **Hyperplane**: In a binary classification problem with two classes, a hyperplane is a decision boundary that separates the data points of one class from those of the other class. For datasets with more than two classes, SVM constructs multiple hyperplanes using various strategies (e.g., one-vs-one, one-vs-all).\n",
    "1. **Margin**: The margin is the distance between the hyperplane and the nearest data points (support vectors) from each class. SVM aims to maximize this margin to achieve better generalization and improve the classifier's robustness to new data points.\n",
    "1. **Kernel Trick**: SVM can handle nonlinear decision boundaries by using a kernel function to map the original input space into a higher-dimensional feature space where the classes are more easily separable. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid kernels.\n",
    "1. **Regularization Parameter (C)**: SVM includes a regularization parameter (C) that controls the trade-off between maximizing the margin and minimizing the classification error. A small C value allows for a wider margin but may lead to misclassification of some training examples, while a large C value prioritizes correctly classifying all training examples but may result in a narrower margin.\n",
    "1. **Support Vectors**: Support vectors are the the subset of training data points that closest to the decision boundary (hyperplane). These points influence the position and orientation of the hyperplane as the hyperplane is determined based on maximizing the margin between these support vectors.\n",
    "\n",
    "SVM is widely used in various applications, including text categorization, image classification, bioinformatics, and financial forecasting. Its effectiveness, especially in high-dimensional spaces, combined with its ability to handle both linear and nonlinear decision boundaries, makes it a versatile and powerful machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a860797-666a-408c-bba3-56ec9816db34",
   "metadata": {},
   "source": [
    "## Why it called a Support Vector Machine?\n",
    "\n",
    "The name \"Support Vector Machine\" emphasizes the algorithm's focus on identifying `support vectors` (critical data points) and constructing an optimal decision boundary (hyperplane) between different classes in the feature space. The term `\"machine\"` underscores the algorithm's role as a learning framework for building classification models based on training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4a9dcd-4165-4b26-9141-977c15ff6796",
   "metadata": {},
   "source": [
    "## What is One-Class SVM?\n",
    "\n",
    "It is a variant of SVM that is primarily used for outlier detection and novelty detection tasks. Unlike traditional SVM, which is typically used for binary classification tasks, One-Class SVM is designed to learn a representation of normal data points and then identify deviations from this representation as outliers.\n",
    "\n",
    "The main idea behind One-Class SVM is to learn a decision boundary (hyperplane) that encapsulates the normal data points in the feature space. This decision boundary is constructed in such a way that it separates the normal data points from the origin or the center of the feature space while maximizing the margin around the normal data. Any data points that fall outside this boundary are considered outliers or anomalies.\n",
    "\n",
    "Key characteristics of One-Class SVM include:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf2cc7-2183-4918-b9bd-0351b3915e51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "1. **Unsupervised Learning**: One-Class SVM is often used in an unsupervised learning setting, where only normal data points are available for training. The algorithm learns to distinguish between normal and abnormal data points based solely on the characteristics of the normal data.\n",
    "1. **Single-Class Classification**: One-Class SVM performs single-class classification, where it learns to model only the distribution of normal data points without explicitly modeling other classes. It aims to identify deviations from the normal data distribution rather than distinguishing between multiple classes.\n",
    "1. **Hyperparameter Tuning**: One-Class SVM includes hyperparameters such as the kernel type, regularization parameter (nu), and kernel parameters (if applicable), which need to be tuned to achieve optimal performance. The choice of hyperparameters can significantly affect the model's ability to detect outliers.\n",
    "1. **Application in Outlier Detection**: One-Class SVM is commonly used for outlier detection tasks, where the goal is to identify data points that deviate significantly from the normal behavior of the dataset. It has applications in various domains, including fraud detection, intrusion detection, and anomaly detection in sensor data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3258370-0533-4b50-b723-7c235153b892",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a62139-f3e7-4bec-b306-0d9060158e6a",
   "metadata": {},
   "source": [
    "### How to re-train a One-Class SVM model using labeled data\n",
    "It trained a OCSVM model, then add some labeled feedback data to train dataset to re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3e9927e-4b7f-4c44-80cf-4f9b354229fe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Performance (Test Score): 12.646710130260601\n",
      "Retrained Model Performance (Test Score): 13.60183430807445\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic labeled data (normal and anomalous)\n",
    "normal_data = np.random.normal(loc=0, scale=1, size=(1000, 2))  # Normal data\n",
    "anomalous_data = np.random.uniform(low=-10, high=10, size=(50, 2))  # Anomalous data\n",
    "\n",
    "# Label the data: 1 for normal, -1 for anomalous\n",
    "normal_labels = np.ones(len(normal_data))\n",
    "anomalous_labels = -np.ones(len(anomalous_data))\n",
    "\n",
    "# Combine the labeled data\n",
    "labeled_data = np.vstack([normal_data, anomalous_data])\n",
    "labels = np.hstack([normal_labels, anomalous_labels])\n",
    "\n",
    "# Split the labeled data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(labeled_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate and train the initial One-Class SVM model\n",
    "ocsvm_model = OneClassSVM(nu=0.1)  # Adjust nu parameter as needed\n",
    "ocsvm_model.fit(X_train)\n",
    "\n",
    "# Evaluate initial model performance\n",
    "initial_score = ocsvm_model.score_samples(X_test)\n",
    "print(\"Initial Model Performance (Test Score):\", initial_score.mean())\n",
    "\n",
    "# Collect feedback data (additional labeled data)\n",
    "feedback_data = np.random.normal(loc=2, scale=1, size=(100, 2))  # Additional normal feedback data\n",
    "feedback_labels = np.ones(len(feedback_data))  # Label the feedback data as normal\n",
    "\n",
    "# Combine the feedback data with the original training data\n",
    "X_train_feedback = np.vstack([X_train, feedback_data])\n",
    "y_train_feedback = np.hstack([y_train, feedback_labels])\n",
    "\n",
    "# Re-train the One-Class SVM model with the feedback data\n",
    "ocsvm_model.fit(X_train_feedback)\n",
    "\n",
    "# Evaluate re-trained model performance\n",
    "retrained_score = ocsvm_model.score_samples(X_test)\n",
    "print(\"Retrained Model Performance (Test Score):\", retrained_score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f27838f-9e93-452b-9faa-c19ad7524b0b",
   "metadata": {},
   "source": [
    "In scikit-learn's OneClassSVM, the `score_samples()` method returns the \"negative\" distance to the decision function, which means that lower values indicate better (more normal) instances, and higher (less negative) values indicate more anomalous instances.\n",
    "\n",
    "In other words:\n",
    "\n",
    "* More negative scores (lower values) indicate that the data points are closer to the decision boundary and are considered more normal or inlier-like.\n",
    "* Less negative or positive scores (higher values) indicate that the data points are further from the decision boundary and are considered more anomalous or outlier-like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dca9cec-d3b0-4d17-a0c5-e5074bc4e963",
   "metadata": {},
   "source": [
    "### Save and reload a trained scikit-learn model\n",
    "We use the `joblib` library to serialize & deserialize scikit-learn objects in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a089eb7-fea0-4426-b93a-1aab8a44f3e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Model Performance (Test Score): 12.646710130260601\n",
      "Retrained Model Performance (Test Score): 13.60183430807445\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "# Assume you have already trained a OneClassSVM model\n",
    "ocsvm_model_save = OneClassSVM(nu=0.1)  # Adjust nu parameter as needed\n",
    "ocsvm_model_save.fit(X_train)\n",
    "\n",
    "# Evaluate initial model performance\n",
    "initial_score = ocsvm_model_save.score_samples(X_test)\n",
    "print(\"Initial Model Performance (Test Score):\", initial_score.mean())\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(ocsvm_model_save, 'ocsvm_model.pkl')\n",
    "\n",
    "# Later, when you want to reload the model:\n",
    "# Load the saved model from the file\n",
    "ocsvm_model_reloaded = joblib.load('ocsvm_model.pkl')\n",
    "ocsvm_model_reloaded.fit(X_train_feedback)\n",
    "\n",
    "# Evaluate re-trained model performance\n",
    "retrained_score = ocsvm_model.score_samples(X_test)\n",
    "print(\"Retrained Model Performance (Test Score):\", retrained_score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a7815-a157-4db2-ba07-37e8d8373750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
